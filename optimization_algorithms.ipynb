{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bisection Method \n",
    "def bisection_method_for_min(f, a, b, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform the Bisection Method to estimate the local minimum of a function.\n",
    "    This assumes the local minimum lies in the interval [a, b].\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): The function for which we are trying to find the local minimum.\n",
    "    a (float): The start of the interval [a, b].\n",
    "    b (float): The end of the interval [a, b].\n",
    "    tol (float): The tolerance level for the minimum (default is 1e-5).\n",
    "    max_iter (int): The maximum number of iterations (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    float: The approximate location of the local minimum within the given tolerance.\n",
    "    \"\"\"\n",
    "    iter_count = 0\n",
    "    while (b - a) / 2.0 > tol and iter_count < max_iter:\n",
    "        # Find the midpoint\n",
    "        midpoint = (a + b) / 2.0\n",
    "        \n",
    "        # Derivative of f(alpha) approximated by central difference method\n",
    "        f_prime = (f(midpoint + tol) - f(midpoint - tol)) / (2 * tol)\n",
    "        \n",
    "        # Print the current step\n",
    "        print(f\"Iteration {iter_count}: a = {a}, b = {b}, midpoint = {midpoint}, f'(midpoint) = {f_prime}\")\n",
    "        \n",
    "        # Check if the derivative is close to 0 (indicating a minimum)\n",
    "        if abs(f_prime) < tol:\n",
    "            return midpoint\n",
    "        \n",
    "        # Narrow down the interval based on the slope\n",
    "        if f_prime > 0:\n",
    "            b = midpoint  # Minimum is to the left\n",
    "        else:\n",
    "            a = midpoint  # Minimum is to the right\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    return (a + b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact Line Search using Bisection Method\n",
    "def line_search_bisection(f, a, b, x0, direction, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform a line search using the bisection method to find the optimal step size (alpha)\n",
    "    for minimizing a function f along a given descent direction.\n",
    "\n",
    "    This function uses the bisection method to find the value of alpha that minimizes the \n",
    "    function f(x0 + alpha * direction). It does this by iteratively refining the interval [a, b]\n",
    "    where the derivative of the function with respect to alpha is close to zero.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): A function that takes (alpha, x0, direction) and returns f(x0 + alpha * direction).\n",
    "    a (float): The lower bound of the interval for alpha.\n",
    "    b (float): The upper bound of the interval for alpha.\n",
    "    x0 (numpy array): The current point in the optimization process (starting point).\n",
    "    direction (numpy array): The descent direction along which the function is minimized.\n",
    "    tol (float): The tolerance level for the derivative of f(alpha). Default is 1e-5.\n",
    "    max_iter (int): The maximum number of iterations allowed for the bisection search. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal alpha that minimizes f(x0 + alpha * direction) within the given tolerance.\n",
    "    \"\"\"\n",
    "    iter_count = 0\n",
    "    while (b - a) / 2.0 > tol and iter_count < max_iter:\n",
    "        midpoint = (a + b) / 2.0\n",
    "        \n",
    "        # Derivative of f(alpha) approximated by central difference method\n",
    "        f_prime = (f(midpoint + tol, x0, direction) - f(midpoint - tol, x0, direction)) / (2 * tol)\n",
    "        \n",
    "        print(f\"Iteration {iter_count}: alpha = {midpoint}, f'(alpha) = {f_prime}\")\n",
    "        \n",
    "        if abs(f_prime) < tol:  # Stop if the derivative is close to zero\n",
    "            return midpoint\n",
    "        \n",
    "        if f_prime > 0:\n",
    "            b = midpoint  # Narrow down to the left\n",
    "        else:\n",
    "            a = midpoint  # Narrow down to the right\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    return (a + b) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtrackign Approximate Line Search\n",
    "def backtracking_line_search(f, grad_f, x_i, d, alpha=1, p=0.5, c=1e-4):\n",
    "    \"\"\"\n",
    "    Backtracking line search algorithm to find the step size that satisfies the Wolfe conditions.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): The objective function.\n",
    "    grad_f (function): The gradient of the objective function.\n",
    "    x_i (numpy array): The current point.\n",
    "    d (numpy array): The descent direction.\n",
    "    alpha (float): The initial step size (default is 1).\n",
    "    p (float): The factor for reducing the step size (default is 0.5).\n",
    "    c (float): The constant for the sufficient decrease condition (default is 1e-4).\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal step size alpha.\n",
    "    \"\"\"\n",
    "    # While the sufficient decrease condition is violated\n",
    "    while f(x_i + alpha * d) > f(x_i) + c * alpha * grad_f(x_i).T.dot(d):\n",
    "        # Reduce the step size\n",
    "        alpha *= p\n",
    "    \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algortithm with Gradient Magnitude Termination Condition\n",
    "def gradient_descent(f, grad_f, x0, epsilon=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm with line search.\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): Objective function to minimize.\n",
    "    grad_f (function): Gradient of the objective function.\n",
    "    x0 (numpy array): Initial guess for the variable.\n",
    "    epsilon (float): Tolerance for stopping.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    x (numpy array): Final variable vector.\n",
    "    f(x) (float): Function value at the final point.\n",
    "    \"\"\"\n",
    "    x_i = x0\n",
    "    for i in range(max_iter):\n",
    "        # Compute the descent direction (negative gradient normalized)\n",
    "        grad = grad_f(x_i)\n",
    "        d_i = -grad / np.linalg.norm(grad)\n",
    "        \n",
    "        # Line search to find step size (using backtracking line search)\n",
    "        alpha_i = backtracking_line_search(f, grad_f, x_i, d_i)\n",
    "        \n",
    "        # Update the variable\n",
    "        x_i = x_i + alpha_i * d_i\n",
    "        \n",
    "        # Check termination condition: gradient magnitude\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    return x_i, f(x_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
