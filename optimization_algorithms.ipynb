{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "from autograd import grad, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bisection Method \n",
    "def bisection_method_for_min(f, a, b, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform the Bisection Method to estimate the local minimum of a function.\n",
    "    This assumes the local minimum lies in the interval [a, b].\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): The function for which we are trying to find the local minimum.\n",
    "    a (float): The start of the interval [a, b].\n",
    "    b (float): The end of the interval [a, b].\n",
    "    tol (float): The tolerance level for the minimum (default is 1e-5).\n",
    "    max_iter (int): The maximum number of iterations (default is 100).\n",
    "\n",
    "    Returns:\n",
    "    float: The approximate location of the local minimum within the given tolerance.\n",
    "    \"\"\"\n",
    "    iter_count = 0\n",
    "    while (b - a) / 2.0 > tol and iter_count < max_iter:\n",
    "        # Find the midpoint\n",
    "        midpoint = (a + b) / 2.0\n",
    "        \n",
    "        # Derivative of f(alpha) approximated by central difference method\n",
    "        f_prime = (f(midpoint + tol) - f(midpoint - tol)) / (2 * tol)\n",
    "        \n",
    "        # Print the current step\n",
    "        print(f\"Iteration {iter_count}: a = {a}, b = {b}, midpoint = {midpoint}, f'(midpoint) = {f_prime}\")\n",
    "        \n",
    "        # Check if the derivative is close to 0 (indicating a minimum)\n",
    "        if abs(f_prime) < tol:\n",
    "            return midpoint\n",
    "        \n",
    "        # Narrow down the interval based on the slope\n",
    "        if f_prime > 0:\n",
    "            b = midpoint  # Minimum is to the left\n",
    "        else:\n",
    "            a = midpoint  # Minimum is to the right\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    return (a + b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact Line Search using Bisection Method\n",
    "def line_search_bisection(f, a, b, x0, direction, tol=1e-5, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform a line search using the bisection method to find the optimal step size (alpha)\n",
    "    for minimizing a function f along a given descent direction.\n",
    "\n",
    "    This function uses the bisection method to find the value of alpha that minimizes the \n",
    "    function f(x0 + alpha * direction). It does this by iteratively refining the interval [a, b]\n",
    "    where the derivative of the function with respect to alpha is close to zero.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): A function that takes (alpha, x0, direction) and returns f(x0 + alpha * direction).\n",
    "    a (float): The lower bound of the interval for alpha.\n",
    "    b (float): The upper bound of the interval for alpha.\n",
    "    x0 (numpy array): The current point in the optimization process (starting point).\n",
    "    direction (numpy array): The descent direction along which the function is minimized.\n",
    "    tol (float): The tolerance level for the derivative of f(alpha). Default is 1e-5.\n",
    "    max_iter (int): The maximum number of iterations allowed for the bisection search. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal alpha that minimizes f(x0 + alpha * direction) within the given tolerance.\n",
    "    \"\"\"\n",
    "    iter_count = 0\n",
    "    while (b - a) / 2.0 > tol and iter_count < max_iter:\n",
    "        midpoint = (a + b) / 2.0\n",
    "        \n",
    "        # Derivative of f(alpha) approximated by central difference method\n",
    "        f_prime = (f(midpoint + tol, x0, direction) - f(midpoint - tol, x0, direction)) / (2 * tol)\n",
    "        \n",
    "        print(f\"Iteration {iter_count}: alpha = {midpoint}, f'(alpha) = {f_prime}\")\n",
    "        \n",
    "        if abs(f_prime) < tol:  # Stop if the derivative is close to zero\n",
    "            return midpoint\n",
    "        \n",
    "        if f_prime > 0:\n",
    "            b = midpoint  # Narrow down to the left\n",
    "        else:\n",
    "            a = midpoint  # Narrow down to the right\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    return (a + b) / 2.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtrackign Approximate Line Search\n",
    "def backtracking_line_search(f, grad_f, x_i, d, alpha=1, p=0.5, c=1e-4):\n",
    "    \"\"\"\n",
    "    Backtracking line search algorithm to find the step size that satisfies the Wolfe conditions.\n",
    "\n",
    "    Parameters:\n",
    "    f (function): The objective function.\n",
    "    grad_f (function): The gradient of the objective function.\n",
    "    x_i (numpy array): The current point.\n",
    "    d (numpy array): The descent direction.\n",
    "    alpha (float): The initial step size (default is 1).\n",
    "    p (float): The factor for reducing the step size (default is 0.5).\n",
    "    c (float): The constant for the sufficient decrease condition (default is 1e-4).\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal step size alpha.\n",
    "    \"\"\"\n",
    "    # While the sufficient decrease condition is violated\n",
    "    while f(x_i + alpha * d) > f(x_i) + c * alpha * grad_f(x_i).T.dot(d):\n",
    "        # Reduce the step size\n",
    "        alpha *= p\n",
    "    \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Algortithm with Gradient Magnitude Termination Condition\n",
    "def gradient_descent(f, x0, epsilon=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm with line search.\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): Objective function to minimize.\n",
    "    grad_f (function): Gradient of the objective function.\n",
    "    x0 (numpy array): Initial guess for the variable.\n",
    "    epsilon (float): Tolerance for stopping.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    x (numpy array): Final variable vector.\n",
    "    f(x) (float): Function value at the final point.\n",
    "    \"\"\"\n",
    "    x_i = x0\n",
    "    grad_f = grad(f)\n",
    "    for i in range(max_iter):\n",
    "        # Compute the descent direction (negative gradient normalized)\n",
    "        grad = grad_f(x_i)\n",
    "        d_i = -grad / np.linalg.norm(grad)\n",
    "        \n",
    "        # Line search to find step size (using backtracking line search)\n",
    "        alpha_i = backtracking_line_search(f, grad_f, x_i, d_i)\n",
    "        \n",
    "        # Update the variable\n",
    "        x_i = x_i + alpha_i * d_i\n",
    "        \n",
    "        # Check termination condition: gradient magnitude\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {i+1} iterations.\")\n",
    "            break\n",
    "    \n",
    "    return x_i, f(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with Noise\n",
    "def gradient_descent_with_noise(f, x0, epsilon=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm with Gaussian noise and fixed step size.\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): Objective function to minimize.\n",
    "    grad_f (function): Gradient of the objective function.\n",
    "    x0 (numpy array): Initial guess for the variable.\n",
    "    epsilon (float): Tolerance for stopping.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    x (numpy array): Final variable vector.\n",
    "    f(x) (float): Function value at the final point.\n",
    "    \"\"\"\n",
    "    x_i = x0\n",
    "    grad_f = grad(f)\n",
    "    for i in range(1, max_iter + 1):\n",
    "        # Compute the descent direction (negative gradient normalized)\n",
    "        grad = grad_f(x_i)\n",
    "        d_i = -grad / np.linalg.norm(grad)\n",
    "        \n",
    "        # Set fixed step size\n",
    "        alpha_i = 1 / i\n",
    "        \n",
    "        # Add Gaussian noise with decreasing standard deviation\n",
    "        noise = np.random.normal(0, 1 / (i**3), size=x_i.shape)\n",
    "        \n",
    "        # Update the variable with noisy descent\n",
    "        x_i = x_i + alpha_i * d_i + noise\n",
    "        \n",
    "        # Check termination condition: gradient magnitude\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "    \n",
    "    return x_i, f(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent  with Momentum\n",
    "def gradient_descent_with_momentum(f, x0, alpha=0.1, beta=0.9, epsilon=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent algorithm with momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    f (function): Objective function to minimize.\n",
    "    grad_f (function): Gradient of the objective function.\n",
    "    x0 (numpy array): Initial guess for the variable.\n",
    "    alpha (float): Step size (default is 0.1).\n",
    "    beta (float): Momentum coefficient (default is 0.9).\n",
    "    epsilon (float): Tolerance for stopping.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    x (numpy array): Final variable vector.\n",
    "    f(x) (float): Function value at the final point.\n",
    "    \"\"\"\n",
    "    x_i = x0\n",
    "    grad_f = grad(f)\n",
    "    v = np.zeros_like(x0)  # Initialize the velocity term\n",
    "    \n",
    "    for i in range(1, max_iter + 1):\n",
    "        # Compute the descent direction (negative gradient)\n",
    "        grad = grad_f(x_i)\n",
    "        \n",
    "        # Update velocity\n",
    "        v = beta * v - alpha * grad\n",
    "        \n",
    "        # Update the variable\n",
    "        x_i = x_i + v\n",
    "        \n",
    "        # Check termination condition: gradient magnitude\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "    \n",
    "    return x_i, f(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method(f, x0, epsilon=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Perform Newton's method to find the minimum of a function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    f : callable\n",
    "        The objective function to minimize. It must take a vector as input and return a scalar.\n",
    "    x0 : ndarray\n",
    "        The initial guess for the optimization, a NumPy array of floats.\n",
    "    epsilon : float, optional (default=1e-6)\n",
    "        The convergence tolerance. Iteration stops when the norm of the gradient is less than epsilon.\n",
    "    max_iter : int, optional (default=1000)\n",
    "        The maximum number of iterations allowed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    x_i : ndarray\n",
    "        The final estimate for the minimum point of the function.\n",
    "    steps : list of ndarray\n",
    "        A list containing the points visited during the optimization process, representing the path of the optimization.\n",
    "    \"\"\"\n",
    "    # Ensure x0 is a float-type NumPy array\n",
    "    x_i = x0\n",
    "    steps = [x_i]\n",
    "    \n",
    "    # Compute gradient and Hessian functions\n",
    "    grad_f = grad(f)\n",
    "    hessian_f = hessian(f)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Compute gradient and Hessian at the current point\n",
    "        grad_i = grad_f(x_i)\n",
    "        hessian_i = hessian_f(x_i)\n",
    "        \n",
    "        # Ensure Hessian is invertible\n",
    "        try:\n",
    "            step = np.linalg.inv(hessian_i) @ grad_i\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Hessian is singular, stopping optimization.\")\n",
    "            break\n",
    "        \n",
    "        # Update step\n",
    "        x_i = x_i - step\n",
    "        steps.append(x_i)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad_i) <= epsilon:\n",
    "            print(f\"Converged after {i + 1} steps\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Max iterations reached without convergence.\")\n",
    "    print(f\"Steps: {steps}\")\n",
    "    print(f\"Final Point: {x_i}, f(x) = {f(x_i)}\")\n",
    "    return x_i,steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
